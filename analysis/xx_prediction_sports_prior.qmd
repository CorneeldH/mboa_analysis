---
    title: "Predictive Modeling for Sports Academy Dropout"
    author: "MBOA Analysis Team"
    date: last-modified
---


    ```{r}
#| label: setup
#| cache: false
#| output: false
#| include: false
#| freeze: false


source("utils/dev_functions.R")
source("utils/manage_packages.R")

load_all()

```


```{r}
# sport_4 <- read_csv(file.path(config::get("data_analysed_dir"),
#                                                    "uitnodigingsregel_train_sport_4.csv"))

data_sport_4 <- readRDS(file.path(config::get("data_analysed_dir"),
                                                   "data_sport_4.rds"))

```


```{r}
#| label: tbl-summarizy-missing-after
#| tbl-cap: "Kwaliteit van de data na bewerkingen (gesorteerd op missende waarden)"

# Edit the data
data_sport_4_geprepareerd <- data_sport_4 |>
    select(-matches("week")) |>
    # Imputate all numeric variables with the mean
    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x))) |>
    # Convert character variables to factor
    mutate(across(where(is.character), as.factor)) |>
    # Convert logical variables to 0 or 1
    mutate(across(where(is.logical), as.integer)) |>
    # Fill in factors missing values with “Unknown”
    mutate(across(where(is.factor), ~ suppressWarnings(
        fct_explicit_na(.x, na_level = "Onbekend")
    ))) |>
    # Remove variables, where there is only 1 value
    select(where( ~ n_distinct(.) > 1)) |>
    # Convert DEELNEMER_BC_uitval to factor for classification modeling
    # First ensure it's represented as 0/1 integers
    mutate(DEELNEMER_BC_uitval = as.integer(DEELNEMER_BC_uitval),
           # Then convert to factor with descriptive labels
           DEELNEMER_BC_uitval = factor(DEELNEMER_BC_uitval, 
                                        levels = c(0, 1),
                                        labels = c("Geen uitval", "Uitval"))) |>
    # Rearrange the columns so that outcome variable is in front
    select(DEELNEMER_BC_uitval, everything())

```

### Splitten

```{r}
#| label: split-data

set.seed(0821)

# Since we have limited data, we'll use a more efficient approach:
# 1. Use all pre-2023 data as training data
# 2. Use cross-validation for model evaluation during training
# 3. Use 2023 data as the final test set

# Split the data based on COHORT_startjaar
data_train <- data_sport_4_geprepareerd |>
  filter(COHORT_startjaar < 2023)

data_test <- data_sport_4_geprepareerd |>
  filter(COHORT_startjaar == 2023)

# Print size info before proceeding
cat("Training data size (pre-2023):", nrow(data_train), "rows\n")
cat("Test data size (2023):", nrow(data_test), "rows\n")

# Create a cross-validation set with 5 folds (instead of 10) to ensure enough data in each fold
# Check if we have enough data in each class for stratification
if(min(table(data_train$DEELNEMER_BC_uitval)) >= 5) {
  data_resamples <- vfold_cv(data_train, v = 5, strata = DEELNEMER_BC_uitval)
  cat("Using 5-fold stratified cross-validation\n")
} else {
  data_resamples <- vfold_cv(data_train, v = 5)
  cat("Warning: Limited data per class, using unstratified 5-fold cross-validation\n")
}

# Print the dimensions of datasets
cat("Training set dimensions:", dim(data_train), "\n")
cat("Test set dimensions:", dim(data_test), "\n")
cat("Number of cross-validation folds:", data_resamples$id |> unique() |> length(), "\n")
```

```{r}
#| label: tbl-split-data
#| tbl-cap: "Verhouding van de uitkomstvariabele in de training- en testset"
#| echo: false

# Training set proportions
data_train_prop <- data_train |> 
  count(DEELNEMER_BC_uitval) |> 
  mutate(Naam = "Trainingset (2021-2022)",
         prop = n / sum(n)) 

# Test set proportions
data_test_prop <- data_test |> 
  count(DEELNEMER_BC_uitval) |> 
  mutate(Naam = "Testset (2023)",
         prop = n / sum(n)) 

# Combine all sets to display in a table
bind_rows(data_train_prop,
          data_test_prop) |> 
  mutate(prop = scales::percent(prop, accuracy = 0.1)) |>
  select(Naam, DEELNEMER_BC_uitval, n, prop) |> 
  knitr::kable(col.names = c("Dataset", "Uitval", "Aantal", "Proportie"))
```

### Bepaal het aantal PC-cores

Omdat een random forest model veel berekeningen vereist, willen we daarvoor alle computerkracht gebruiken die beschikbaar is. Het aantal CPU's (*cores*), wat verschilt per computer, bepaalt hoe snel het model getraind kan worden. We bepalen het aantal cores en gebruiken dat bij het bouwen van het model.

```{r}
#| label: cores

# Determine the number of cores
cores <- parallel::detectCores()

```

### Maak het model

We bouwen eerst het model. We gebruiken de `rand_forest` functie om het model te bouwen. We tunen de `mtry` en `min_n` parameters. De `mtry` parameter bepaalt het aantal variabelen dat per boom wordt gebruikt. De `min_n` parameter bepaalt het minimum aantal observaties dat in een blad van de boom moet zitten. De functie `tune()` is hier nog een *placeholder* om de beste waarden voor deze parameters - die we later bepalen - in te kunnen stellen. We gebruiken 1.000 bomen c.q. versies van het model.

```{r}
#| label: rf-mod
#| code-fold: false

# Build the model: random forest
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |> 
  set_engine("ranger", num.threads = cores) |> 
  set_mode("classification")
```

### Maak de recipe

We maken een recipe voor het random forest model. We verwijderen variabelen uit de data die niet moeten worden gebruikt in het model zoals identificatienummers en datums. We voegen ook een stap toe om eventuele onbekende categorieën in factoren te verwerken.

```{r}
#| label: rf-recipe
#| code-fold: false

# Create the recipe: random forest
rf_recipe <- 
  recipe(DEELNEMER_BC_uitval ~ ., data = data_train) |> 
  # Remove variables that shouldn't be used in the model
  step_rm(DEELNEMER_ID, 
          matches("_datum"), 
          matches("_begindatum"), 
          matches("_einddatum"),
          COHORT_startjaar,
          DEELNEMER_BC_inschrijvingsduur) |>
  # Create proper handling for any factor variables with NAs
  step_unknown(all_nominal_predictors()) |>
  # Normalize numeric predictors (optional for random forest)
  step_normalize(all_numeric_predictors())
```

### Maak de workflow

We voegen het model en de recipe toe aan de workflow voor dit model.

```{r}
#| label: rf-workflow
#| code-fold: false

# Create the workflow: random forest
rf_workflow <- 
  workflow() |> 
  add_model(rf_mod) |> 
  add_recipe(rf_recipe)

# Show workflow
rf_workflow
```

### Tune en train het model

We trainen en tunen het model in de workflow. We maken een grid met verschillende waarden voor de parameters `mtry` en `min_n`. We gebruiken de Area under the ROC Curve (AUC/ROC) als performance metric. Met de resultaten van de tuning kiezen we het beste model.

```{r}
#| label: rf-tune
#| code-fold: false

# Show the parameters that can be tuned
rf_mod

# Extract the parameters being tuned
extract_parameter_set_dials(rf_mod)

# Determine the seed
set.seed(2904)

# Build the grid: random forest
rf_res <- 
  rf_workflow |> 
  tune_grid(data_resamples,  # Use cross-validation resamples instead of validation set
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```


### Kies het beste model

We evalueren de beste modellen en maken een ROC curve om de performance van het model te visualiseren. Vervolgens vergelijken we de prestaties van de modellen en kiezen daaruit het beste model.

```{r}
#| label: tbl-rf-results
#| tbl-cap: "Model performance voor random forest"
#| code-fold: false

# Show the best models
rf_res |> 
  show_best(metric = "roc_auc", n = 15) |> 
  mutate(mean = round(mean, 6)) |>
  knitr::kable(col.names = c("Mtry", 
                             "Min. aantal", 
                             "Metriek",
                             "Estimator",
                             "Gemiddelde",
                             "Aantal",
                             "SE",
                             "Configuratie"))

```

```{r}
#| label: fig-rf-results
#| fig-cap: "Model performance random forest"

# Plot the results
autoplot_obj <- autoplot(rf_res) +
  theme_minimal() +
  labs(
    y = "roc/auc",
    caption = "Training op data 2021-2022, test op data 2023"
  )
  
# Check if add_theme_elements exists
if(exists("add_theme_elements")) {
  autoplot_obj <- add_theme_elements(autoplot_obj, title_subtitle = FALSE)
}

print(autoplot_obj)

```

```{r}
#| label: rf-best
#| code-fold: false

# Select the best model
rf_best <- 
  rf_res |> 
  select_best(metric = "roc_auc")

```


```{r}
#| label: tbl-ref-best
#| tbl-cap: "Hoogste model performance voor random forest"

rf_best |> 
  knitr::kable(col.names = c("Mtry", 
                             "Min. aantal", 
                             "Configuratie"))

```

```{r}
#| label: tbl-rf-predictions
#| tbl-cap: "Predicties voor random forest"

# Collect the predictions
# For debugging, let's first check the actual column names
pred_columns <- rf_res |> 
  collect_predictions() |> 
  colnames()
print(paste("Prediction columns:", paste(pred_columns, collapse=", ")))

# Now collect and format predictions with correct column names
rf_res |> 
  collect_predictions() |> 
  head(10) |>
  # Use backticks for columns with spaces
  mutate(across(starts_with(".pred_"), ~scales::percent(., accuracy = 0.1))) |>
  knitr::kable(col.names = c("% Geen uitval", 
                             "% Uitval", 
                             "ID",
                             "Rij",
                             "Mtry", 
                             "Min. aantal", 
                             "Uitval",
                             "Configuratie"))
```

```{r}
#| label: fig-rf-auc
#| fig-cap: "ROC curve voor random forest"
 
# Determine the AUC/ROC curve
# First check column names for debugging
pred_cols <- rf_res |>
  collect_predictions(parameters = rf_best) |>
  colnames()
print(paste("Available columns for ROC curve:", paste(pred_cols, collapse=", ")))

# Get the correct prediction column name for "Uitval" (the positive class)
pred_cols_uitval <- pred_cols[grep("\\.pred_.*[Uu]itval$", pred_cols)]
pred_cols_all <- pred_cols[grep("^\\.pred_", pred_cols)]

print(paste("All prediction columns:", paste(pred_cols_all, collapse=", ")))

# Select specifically the column for "Uitval"
if(length(pred_cols_uitval) > 0) {
  # If multiple matches, take the first one
  pred_col_uitval <- pred_cols_uitval[1]
  print(paste("Found Uitval column:", pred_col_uitval))
} else if(length(pred_cols_all) >= 2) {
  # If no specific match but we have .pred_ columns, use the second one
  # (typically the positive class probability)
  pred_col_uitval <- pred_cols_all[2]
  print(paste("Using second prediction column:", pred_col_uitval))
} else if(length(pred_cols_all) == 1) {
  # Just use whatever prediction column we have
  pred_col_uitval <- pred_cols_all[1]
  print(paste("Using available prediction column:", pred_col_uitval))
} else {
  # Last resort fallback
  pred_col_uitval <- ".pred_class"
  print("WARNING: Could not identify any prediction column, using .pred_class as fallback")
}

# Ensure we have exactly one column name
if(length(pred_col_uitval) != 1) {
  stop("Error: Could not identify a single prediction column for the positive class")
}

# Now calculate ROC curve with the correct column
rf_auc <- 
  rf_res |> 
  collect_predictions(parameters = rf_best) |> 
  roc_curve(DEELNEMER_BC_uitval, !!sym(pred_col_uitval)) |>  # Use dynamic column name
  mutate(model = "Random Forest")

# Plot the ROC curve
# Check if get_roc_plot is available, otherwise use ggplot2 directly
if(exists("get_roc_plot")) {
  get_roc_plot(rf_auc, position = 2)
} else {
  # Basic ROC curve plot with ggplot2
  ggplot(rf_auc, aes(x = 1 - specificity, y = sensitivity)) +
    geom_path(size = 1.2, color = "steelblue") +
    geom_abline(lty = 3) +
    labs(title = "ROC Curve - Random Forest Model") +
    theme_minimal()
}

# Determine the AUC of the best model
rf_auc_highest   <-
  rf_res |>
  collect_predictions(parameters = rf_best) |> 
  roc_auc(DEELNEMER_BC_uitval, !!sym(pred_col_uitval))  # Use the detected prediction column

# Create or update df_model_results (create it if it doesn't exist)
if(!exists("df_model_results")) {
  df_model_results <- tibble(model = character(), auc = numeric())
}

# Add model name and AUC to df_model_results
df_model_results <- 
  df_model_results |>
  add_row(model = "Random Forest", 
          auc = rf_auc_highest$.estimate)

```


## De uiteindelijke fit

-   In de laatste stap van deze analyse maken we het model definitief.
-   We testen het model op de testset en evalueren het model met metrieken en de Variable Importance (VI). De VI kwantificeert de bijdrage van elke variabele aan de voorspellende kracht van een model. Het identificeert welke variabelen significant zijn voor de modelprestaties, wat essentieel is voor het interpreteren en optimaliseren van een model [@VanderLaan.2006]. Methoden zoals de Shapley-waarde en permutation importance worden vaak toegepast om dit belang te meten. Op deze methoden komen we terug in het volgende hoofdstuk.

### Combineer de AUC/ROC curves en kies het beste model

Eerst combineren we de AUC/ROC curves van de modellen om ze te vergelijken. We kiezen het beste model op basis van de hoogste AUC/ROC.



```{r}
#| label: best-model-auc-roc

# Determine which of the models is best based on highest AUC/ROC
df_model_results <- df_model_results |>
  mutate(number = row_number()) |> 
  mutate(best = ifelse(auc == max(auc), TRUE, FALSE)) |> 
  arrange(number)

# Determine the best model
best_model     <- df_model_results$model[df_model_results$best == TRUE]
best_model_auc <- round(df_model_results$auc[df_model_results$best == TRUE], 4)
```


We maken het finale model op basis van de beste parameters die we hebben gevonden. Door in de engine bij `importance` de `impurity` op te geven, wordt het beste random forest model gekozen om de data definitief mee te classificeren.

```{r}
#| label: last-mod
#| code-fold: false

# Test the developed model on the test set
# Determine the optimal parameters

last_rf_mod <-
  rand_forest(mtry = rf_best$mtry,
              min_n = rf_best$min_n,
              trees = 1000) |>
  set_engine("ranger", num.threads = cores, importance = "impurity") |>
  set_mode("classification")

```


### Maak de workflow

We voegen het model toe aan de workflow en updaten de workflow met het finale model.

```{r}
#| label: last-workflow
#| code-fold: false

# Update the workflows
last_rf_workflow <- 
  rf_workflow |> 
  update_model(last_rf_mod)

```


### Fit het finale model

We voeren de finale fit uit. De functie `last_fit` past het model toe op de validatieset.

```{r}
#| label: last-fit
#| code-fold: false

# Perform the final fit
set.seed(2904)

# Create a split object from training and test data
split_obj <- rsample::initial_split(
  data_sport_4_geprepareerd,
  prop = nrow(data_train) / nrow(data_sport_4_geprepareerd),
  strata = "DEELNEMER_BC_uitval"
)

# Make a final fit with the best model
last_fit_rf <- 
  last_rf_workflow |> 
  last_fit(split_obj)

# Save the model and results using the project's save functions
save_analysed(last_fit_rf, filename = "sports_prediction_model_fit")
save_analysed(df_model_results, filename = "sports_prediction_model_results")

```


### Evalueer het finale model: metrieken en variable importance

We evalueren het finale model nu grondiger op basis van 4 metrieken: 1) accuraatheid, 2) ROC/AUC en 3) de [Brier score](https://en.wikipedia.org/wiki/Brier_score) (de Mean Squared Error). Het is zinvol om accuraatheid, ROC/AUC en de Brier-score pas bij het finale model toe te passen, omdat dit efficiënter is en overfitting voorkomt. Zo combineren we een snelle modelist_selectie met een grondige evaluatie van het uiteindelijke model.

```{r}
#| label: last-fit-metrics-vi
#| code-fold: false

# Collect the metrics
last_fit_rf |> 
  collect_metrics() |> 
  mutate(.estimate = round(.estimate, 4)) |>
  knitr::kable(col.names = c("Metriek", 
                             "Estimator",
                             "Estimate",
                             "Configuratie"))

# Extract variable importance
if(inherits(try(extract_fit_parsnip(last_fit_rf)), "try-error")) {
  cat("Unable to extract model for variable importance.\n")
} else {
  last_fit_rf |> 
    extract_fit_parsnip() |> 
    vi() |>
    #head(10) |>
    knitr::kable(caption = "Top 10 belangrijkste variabelen")
}

```


## Conclusies

```{r}
#| label: conclusions-accuracy

# Calculate baseline accuracy (predicting most common class)
class_distribution <- data_test |>
  count(DEELNEMER_BC_uitval) |>
  mutate(pct = n/sum(n))

most_common_class <- class_distribution |>
  filter(pct == max(pct)) |>
  pull(DEELNEMER_BC_uitval)

baseline_accuracy <- class_distribution |>
  filter(DEELNEMER_BC_uitval == most_common_class) |>
  pull(pct)

# Get model accuracy
model_accuracy <- last_fit_rf |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  pull(.estimate)

# Calculate lift
accuracy_lift <- model_accuracy - baseline_accuracy
```

### Samenvatting resultaten

In dit onderzoek hebben we een predictief model ontwikkeld om uitval bij studenten van de Sportacademie te voorspellen. Het model gebruikt gegevens van eerdere jaren (2021-2022) als trainingdata en 2023 als testdata.

- **Base model accuraatheid:** `r round(baseline_accuracy*100, 1)`% (voorspellen van de meest voorkomende klasse)
- **Model accuraatheid:** `r round(model_accuracy*100, 1)`%
- **Lift:** `r round(accuracy_lift*100, 1)`% verbetering ten opzichte van het base model

De belangrijkste factoren die bijdragen aan het voorspellen van uitval zijn te zien in de variabele-importantietabel hierboven. Deze inzichten kunnen worden gebruikt om gerichte interventies te ontwikkelen voor studenten met een hoger uitvalrisico.

### Volgende stappen

1. Verder verfijnen van het model met aanvullende variabelen
2. Ontwikkelen van een dashboard voor real-time monitoring
3. Implementeren van vroegtijdige interventies gebaseerd op de voorspellingen

```{r}
#| echo: false
# Save prepared data for future reference
save_analysed(data_sport_4_geprepareerd, filename = "sports_prepared_data")
```
