---
title: "Model Interpretation"
author: "MBOA Analysis Team"
date: last-modified
---

```{r}
#| label: setup
#| cache: false
#| output: false
#| include: false
#| freeze: false

source("utils/dev_functions.R")
source("utils/manage_packages.R")

# Load all functions from the R package
load_all()

```

## Introductie

Dit document interpreteert de voorspellende modellen voor studentuitval die in het vorige document zijn getraind. We analyseren:

1. Variabele-importantie over verschillende modellen
2. Vergelijking van modelprestaties
3. Belangrijkste factoren die studentuitval beïnvloeden
4. Programma-specifieke inzichten

## Laden van modelresultaten

Eerst laden we de getrainde modellen en hun prestatiemetrieken.

```{r}
#| label: load-model-results

# Get model settings from config
model_settings <- config::get("model_settings")

# Define week strategies
week_strategies <- c("none", "early", "all")

# Get directory paths
models_dir <- file.path(config::get("modelled_dir"), "run", "models")
metrics_dir <- file.path(config::get("modelled_dir"), "run", "metrics")

# Try to load all model results first
all_models_path <- file.path(config::get("modelled_dir"), "run", "all_model_results.rds")
if(file.exists(all_models_path)) {
  all_model_results <- readRDS(all_models_path)
  cat("Loaded combined results for", length(all_model_results), "models\n")
} else {
  cat("Combined model results not found. Will load individual files instead.\n")
  all_model_results <- NULL
}

# Load performance summary
performance_summary_path <- file.path(metrics_dir, "performance_summary.rds")
if(file.exists(performance_summary_path)) {
  performance_summary <- readRDS(performance_summary_path)
  
  # Extract program-level and week strategy if not already present
  if(!("program_level" %in% names(performance_summary))) {
    performance_summary <- performance_summary |>
      mutate(
        program_level = sapply(strsplit(dataset_id, "_weeks_"), function(x) x[1]),
        week_strategy = sapply(strsplit(dataset_id, "_weeks_"), function(x) x[2])
      )
  }
  
  # Create a display version of the summary
  performance_display <- performance_summary |>
    select(dataset_id, program_level, week_strategy, .metric, .estimate) |>
    pivot_wider(
      names_from = .metric,
      values_from = .estimate
    ) |>
    arrange(desc(roc_auc))
  
  cat("Loaded performance summary for", n_distinct(performance_summary$dataset_id), "models\n")
  cat("Models by week strategy:\n")
  print(table(performance_summary$week_strategy[performance_summary$.metric == "roc_auc"]))
  
  # Show top 5 models by ROC AUC (better for ranking)
  cat("\nTop 5 models by ROC AUC:\n")
  print(knitr::kable(head(performance_display |> arrange(desc(roc_auc)), 5), digits = 4))
} else {
  cat("Performance summary not found. Looking for individual metrics files...\n")
  
  # Find metrics files
  metrics_files <- list.files(metrics_dir, pattern = "_metrics\\.rds$", full.names = TRUE)
  
  if(length(metrics_files) > 0) {
    # Load individual metrics files
    performance_summary <- data.frame()
    
    for(metrics_file in metrics_files) {
      # Load metrics
      metrics <- readRDS(metrics_file)
      
      # Extract dataset ID from filename
      dataset_id <- gsub("_metrics\\.rds$", "", basename(metrics_file))
      
      # Extract program-level and week strategy
      parts <- strsplit(dataset_id, "_weeks_")[[1]]
      if(length(parts) >= 2) {
        program_level <- parts[1]
        week_strategy <- parts[2]
      } else {
        # Handle case where week strategy is not in filename
        program_level <- dataset_id
        week_strategy <- "unknown"
      }
      
      # Add to summary
      if(!is.null(metrics$metrics)) {
        metrics_df <- metrics$metrics |>
          mutate(
            dataset_id = dataset_id,
            program_level = program_level,
            week_strategy = week_strategy
          )
        
        performance_summary <- bind_rows(performance_summary, metrics_df)
      }
    }
    
    if(nrow(performance_summary) > 0) {
      # Create a display version of the summary
      performance_display <- performance_summary |>
        select(dataset_id, program_level, week_strategy, .metric, .estimate) |>
        pivot_wider(
          names_from = .metric,
          values_from = .estimate
        ) |>
        arrange(desc(roc_auc))
      
      cat("Loaded metrics for", n_distinct(performance_summary$dataset_id), "models\n")
      print(knitr::kable(head(performance_display, 5), digits = 4))
    } else {
      cat("No model metrics found.\n")
    }
  } else {
    cat("No metrics files found.\n")
    performance_summary <- NULL
    performance_display <- NULL
  }
}

# Try to load week strategy comparison if available
week_comparison_path <- file.path(metrics_dir, "week_strategy_comparison.rds")
if(file.exists(week_comparison_path)) {
  week_comparison <- readRDS(week_comparison_path)
  
  cat("\nLoaded direct comparison for", nrow(week_comparison), "program-levels with all week strategies\n")
  
  # Summarize week comparison
  week_comparison_summary <- week_comparison |>
    summarize(
      avg_none = mean(none, na.rm = TRUE),
      avg_early = mean(early, na.rm = TRUE),
      avg_all = mean(all, na.rm = TRUE),
      avg_early_vs_none = mean(early_vs_none, na.rm = TRUE),
      avg_all_vs_none = mean(all_vs_none, na.rm = TRUE),
      avg_all_vs_early = mean(all_vs_early, na.rm = TRUE)
    )
  
  cat("\nAverage performance across strategies:\n")
  print(knitr::kable(week_comparison_summary, digits = 4))
  
  # Visualize comparison
  week_comparison_long <- week_comparison |>
    select(program_level, none, early, all) |>
    pivot_longer(
      cols = c(none, early, all),
      names_to = "week_strategy",
      values_to = "roc_auc"
    ) |>
    mutate(
      week_strategy = factor(week_strategy, 
                          levels = c("none", "early", "all"),
                          labels = c("Bij start", "Na 5 weken", "Na 10 weken"))
    )
  
  ggplot(week_comparison_long, aes(x = week_strategy, y = roc_auc, group = program_level)) +
    geom_line(alpha = 0.3, aes(color = program_level)) +
    geom_point(alpha = 0.3, aes(color = program_level)) +
    #stat_summary(fun = mean, geom = "point", size = 4, color = "red") +
    stat_summary(fun = mean, geom = "line", linewidth = 1.5, color = "black", group = 1) +
    labs(
      title = "Model ROC AUC over tijd",
      x = "Tijdstip",
      y = "ROC AUC",
      color = "Programma-niveau"
    ) +
    scale_y_continuous(labels = scales::percent) +
    theme_minimal() +
    theme(legend.position = "none")
} else {
  cat("\nNo week strategy comparison found.\n")
}
```

## Interpreteren van geselecteerde modellen

We interpreteren de best presterende modellen om de belangrijke factoren te begrijpen.

```{r}
#| label: interpret-models

# Initialize results list
interpretation_results <- list()

# Select models to interpret
if(exists("performance_display") && nrow(performance_display) > 0) {
  if(exists("week_comparison") && nrow(week_comparison) > 0) {
    # Find top 3 program-levels with best overall performance
    top_program_levels <- week_comparison |>
      mutate(avg_roc_auc = (none + early + all)/3) |>
      arrange(desc(avg_roc_auc)) |>
      pull(program_level)
    
    # Find all models for these program-levels
    top_program_models <- performance_display |>
      filter(program_level %in% top_program_levels) |>
      pull(dataset_id)
  } else {
    # If no week comparison, select top 3 models overall by ROC AUC
    top_program_models <- head(performance_display |> arrange(desc(roc_auc)) |> pull(dataset_id), 3)
  }
  
  # Combine both selection approaches
  selected_models <- c(unlist(top_by_strategy), top_program_models)
  selected_models <- unique(selected_models)
  
  cat("Selected", length(selected_models), "models for interpretation:\n")
  for(model_id in selected_models) {
    parts <- strsplit(model_id, "_weeks_")[[1]]
    program_level <- parts[1]
    week_strategy <- parts[2]
    cat("- ", model_id, " (", program_level, ", weeks: ", week_strategy, ")\n", sep="")
  }
  
  # Interpret each selected model
  for(model_id in selected_models) {
    # Extract program-level and week strategy
    parts <- strsplit(model_id, "_weeks_")[[1]]
    program_level <- parts[1]
    week_strategy <- parts[2]
    
    cat("\n===== Interpreting model:", model_id, "=====\n")
    cat("Program-level:", program_level, "| Week strategy:", week_strategy, "\n")
    
    # First try to get model from combined results
    model <- NULL
    if(!is.null(all_model_results) && model_id %in% names(all_model_results)) {
      model_results <- all_model_results[[model_id]]
      cat("Using model from combined results\n")
    } else {
      # If not in combined results, load individual model file
      model_path <- file.path(models_dir, paste0(model_id, "_model.rds"))
      
      if(file.exists(model_path)) {
        # Load the model
        model <- readRDS(model_path)
        cat("Loaded model from individual file\n")
        
        # Get metrics
        model_metrics <- performance_summary |>
          filter(dataset_id == model_id)
        
        # Create model results structure expected by interpret_model function
        model_results <- list(
          final_model = model,  # The model itself is the workflow
          metrics = model_metrics,
          filter_info = list(
            program = program_level,
            level = gsub("^.*_level([0-9]+).*$", "\\1", program_level),
            week_vars = week_strategy
          )
        )
      } else {
        cat("  ERROR: Model file not found:", model_path, "\n")
        next
      }
    }
    
    # Run interpretation
    tryCatch({
      interpretation <- interpret_model(
        model_results = model_results,
        n_vars = 15,
        save = TRUE
      )
      
      # Store in results list
      interpretation_results[[model_id]] <- interpretation
      
      # Display performance metrics
      # Primary metric: ROC AUC (for ranking capability)
      model_roc_auc <- performance_display |>
        filter(dataset_id == model_id) |>
        pull(roc_auc)
      
      cat("  Model ROC AUC:", scales::percent(model_roc_auc, accuracy = 0.1), 
          "(higher values = better student ranking)\n")
      
      # Secondary metric: accuracy (for reference)
      if ("accuracy" %in% colnames(performance_display)) {
        model_accuracy <- performance_display |>
          filter(dataset_id == model_id) |>
          pull(accuracy)
        
        cat("  Model accuracy:", scales::percent(model_accuracy, accuracy = 0.1), "\n")
      }
      
      # Display variable importance
      if(!is.null(interpretation$variable_importance) && nrow(interpretation$variable_importance) > 0) {
        cat("\n  Top 10 important variables:\n")
        print(knitr::kable(head(interpretation$variable_importance, 10)))
        
      } else {
        cat("  No variable importance available.\n")
      }
      
    }, error = function(e) {
      cat("  ERROR interpreting model:", e$message, "\n")
    })
  }
  
  # Save all interpretation results
  if(length(interpretation_results) > 0) {
    saveRDS(interpretation_results, 
            file.path(config::get("modelled_dir"), "interpreted", "all_interpretations.rds"))
    
    cat("\nSaved interpretations for", length(interpretation_results), "models\n")
  }
} else {
  cat("No models available for interpretation.\n")
}
```

## Variabele-importantie over modellen

We vergelijken variabele-importantie over verschillende modellen om consistente patronen te identificeren.

```{r}
#| label: compare-importance

if(length(interpretation_results) >= 2) {
  # Compare importance across all models
  importance_comparison <- compare_group_importance(
    interpretation_list = interpretation_results,
    n_vars = 15,
    save = TRUE
  )
  
  # Display overall comparison
  if(!is.null(importance_comparison) && nrow(importance_comparison) > 0) {
          # Hier kun je de variabelenamen vervangen voordat je ze weergeeft
    # Laad je variabelenmapping
    mapping_file <- file.path(config::get("data_reference_dir", "data/reference"), "variable_descriptions.csv")
    var_mapping <- NULL
    
    if(file.exists(mapping_file)) {
      var_mapping <- readr::read_delim(mapping_file, delim = ";", show_col_types = FALSE)
      
      # Vervang de variabelenamen in de Variable kolom
      importance_comparison$Variable <- sapply(importance_comparison$Variable, function(var_name) {
        idx <- match(var_name, var_mapping$variable_name)
        if(!is.na(idx)) var_mapping$user_friendly_name[idx] else var_name
      })
    }
    
    cat("\nVariable importance comparison across all models:\n")
    print(knitr::kable(importance_comparison, digits = 4))
    
    # Extract model metadata for better visualization
    model_metadata <- data.frame(
      model_id = names(interpretation_results),
      program_level = sapply(strsplit(names(interpretation_results), "_weeks_"), function(x) x[1]),
      week_strategy = sapply(strsplit(names(interpretation_results), "_weeks_"), function(x) x[2])
    )
    
    # Create a long format for visualization with metadata
    # Ensure we're using user-friendly variable names from the mapping file
    importance_long <- importance_comparison |>
      pivot_longer(
        cols = -Variable,
        names_to = "model_id",
        values_to = "importance"
      ) |>
      left_join(model_metadata, by = "model_id") |>
      mutate(
        Variable = fct_reorder(Variable, importance, .fun = max, na.rm = TRUE, .na_rm = TRUE),
        importance = replace_na(importance, 0),
        week_strategy = factor(week_strategy, 
                             levels = c("none", "early", "all"),
                             labels = c("Bij start", "Na 5 weken", "Na 10 weken"))
      )
    
    # Overall top variables across all models (traditional comparison)
    p2 <- ggplot(importance_long, aes(x = Variable, y = importance, fill = week_strategy)) +
      geom_col(position = "dodge") +
      coord_flip() +
      labs(
        title = "Vergelijking van belang variabele over tijd",
        x = NULL,
        y = "Belang"
      ) +
      scale_fill_discrete(name = "Tijdstip") + 
      theme_minimal() +
      theme(legend.position = "bottom")
    
    print(p2)
    
    # Save overall comparison plot
    ggsave(
      file.path(config::get("modelled_dir"), "interpreted", "visualizations", "importance_comparison.png"),
      p2,
      width = 10,
      height = 8
    )
    
    # We've removed the week variables importance section as requested
  } else {
    cat("\nNo variable importance comparison available.\n")
  }
} else {
  cat("\nNeed at least 2 models for comparison. Skipping.\n")
}
```

## Vergelijking van modelprestaties

Laten we prestatiemetrieken vergelijken tussen verschillende modellen.

```{r}
#| label: compare-performance

if(exists("performance_display") && nrow(performance_display) > 0) {
  # For ROC AUC, we don't need to calculate baseline from data
  # A random model has ROC AUC of 0.5 regardless of class distribution
  
  # Display performance sorted by ROC AUC
  cat("\nModel performance by ROC AUC (ranking capability):\n")
  print(knitr::kable(
    performance_display |> 
      select(dataset_id, program_level, week_strategy, roc_auc) |>
      arrange(desc(roc_auc)),
    digits = 4
  ))
  
  # Plot ROC AUC comparison with fixed baseline of 0.5
  model_comparison <- performance_display |>
    select(dataset_id, program_level, week_strategy, roc_auc) |>
    mutate(
      baseline_auc = 0.5,  # Random model has ROC AUC of 0.5
      auc_gain = roc_auc - baseline_auc
    )
  
  # Plot performance comparison
  p <- ggplot(model_comparison, aes(x = reorder(dataset_id, roc_auc))) +
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey70") +
    geom_col(aes(y = roc_auc), fill = "steelblue", alpha = 0.7) +
    geom_text(aes(y = roc_auc, label = sprintf("%.3f", roc_auc)), 
              hjust = -0.2) +
    coord_flip() +
    labs(
      title = "Vergelijking van model ROC AUC",
      subtitle = "Hogere waarden geven betere studentenrangschikking aan (0.5 = willekeurige rangschikking)",
      x = NULL,
      y = "ROC AUC"
    ) +
    theme_minimal() +
    ylim(0, max(1, max(model_comparison$roc_auc) * 1.1))
  
  print(p)
  
  # Save plot
  ggsave(
    file.path(config::get("modelled_dir"), "interpreted", "visualizations", "roc_auc_comparison.png"),
    p,
    width = 10,
    height = 8
  )
}
```

## Interventieplanning met waarschijnlijkheidsrangschikking

Om interventies te plannen op basis van uitvalrisico, maken we liftgrafieken die laten zien hoeveel studenten moeten worden benaderd om een bepaald percentage uitvallers te bereiken.

```{r}
#| label: program-comparison-chart
#| fig.width: 10
#| fig.height: 6

# Parameter: specify the program name pattern to analyze
program_name <- "Keuken.*25180.*level2"  # Can be changed to any program pattern
program_name <- "Leidinggeven.*25160.*level4"
program_name <- "Dienstverlening (25498)_level2"
#program_name <- "Pedagogisch (25698)_level4"
#program_name <- "Business (25727)_level4"
#program_name <- "Sociaal (25615)_level4"


# Create a function to generate comparison chart for any program
create_program_comparison <- function(program_name) {
    program_pattern <- str_replace(program_name, " \\(", ".*")
    program_pattern <- str_replace(program_pattern, "\\)_", ".*")
    
  # Find models matching the pattern
  model_ids <- names(all_model_results)[str_detect(names(all_model_results), program_pattern)]
  
  if(length(model_ids) == 0) {
    cat("Models for", program_pattern, "not found\n")
    return(NULL)
  }
  
  # Extract program name from first matching model for display
  program_display_name <- sub("_weeks.*$", "", model_ids[1])
  
  # Initialize data collection
  chart_data <- data.frame()
  
  # Extract data from each model
  for(model_id in model_ids) {
    model <- all_model_results[[model_id]]
    week_strategy <- sub(".*weeks_", "", model_id)
    
    # Extract predictions
    predictions <- tryCatch(
      collect_predictions(model$final_model), 
      error = function(e) NULL
    )
    
    if(!is.null(predictions) && nrow(predictions) > 0) {
      # Find outcome and probability columns
      outcome_col <- grep("DEELNEMER_BC_uitval|.outcome|truth", colnames(predictions), value = TRUE)[1]
      prob_col <- grep("^.pred_Uitval|^.pred_TRUE", colnames(predictions), value = TRUE)[1]
      
      if(!is.na(outcome_col) && !is.na(prob_col)) {
        # Calculate lift data
        lift_data <- predictions |>
          arrange(desc(!!sym(prob_col))) |>
          mutate(
            actual_dropout = as.numeric(!!sym(outcome_col) %in% c("Uitval", "TRUE", "1", 1, TRUE)),
            rank = row_number(),
            rank_pct = rank / n(),
            cum_dropouts = cumsum(actual_dropout),
            cum_dropout_pct = cum_dropouts / sum(actual_dropout)
          ) |>
          select(rank_pct, cum_dropout_pct) |>
          mutate(strategy = week_strategy)
          
        chart_data <- bind_rows(chart_data, lift_data)
      }
    }
  }
  
  # Create chart if we have data
  if(nrow(chart_data) > 0) {
    # Format strategy names
    chart_data <- chart_data |>
      mutate(strategy = factor(strategy, 
                            levels = c("none", "early", "all"),
                            labels = c("Bij start", "Na 5 weken", "Na 10 weken")))
    
    # Clean program name for display
    clean_program <- gsub("\\.\\.+", " ", program_display_name)
    clean_program <- str_replace(clean_program , "_level", " niveau ")
    
    # Create the chart
    comparison_chart <- ggplot(chart_data, aes(x = rank_pct, y = cum_dropout_pct, color = strategy)) +
      #geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray70") +
      geom_line(linewidth = 1.2) +
      labs(
        title = clean_program,
        subtitle = "Toont % uitvallers bereikt bij interventie met % studenten",
        x = "Percentage benaderde studenten",
        y = "Percentage bereikte uitvallers",
        color = "Tijdstip",
        caption = paste("Programma:", clean_program, "| Gerangschikt op ROC AUC")
      ) +
      scale_x_continuous(labels = scales::percent) +
      scale_y_continuous(labels = scales::percent) +
      theme_minimal() +
      theme(
        plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", size = 12),
        legend.position = "bottom"
      )
    
    # Save chart
    vis_dir <- file.path(config::get("modelled_dir"), "interpreted", "visualizations", "programs")
    if(!dir.exists(vis_dir)) {
      dir.create(vis_dir, recursive = TRUE)
    }
    
    # Create clean filename
    clean_filename <- gsub("[^a-zA-Z0-9]", "_", clean_program)
    filename <- paste0("lift_chart_", clean_filename, "_comparison.png")
    
    ggsave(
      file.path(vis_dir, filename),
      comparison_chart,
      width = 10,
      height = 7
    )
    
    return(comparison_chart)
  } else {
    cat("Could not extract prediction data for", program_pattern, "models\n")
    return(NULL)
  }
}

# Run the function with the specified program
program_chart <- create_program_comparison(program_name)

# Print the chart if available
if(!is.null(program_chart)) {
  print(program_chart)
}
```

Deze liftgrafieken bieden een praktisch hulpmiddel voor het plannen van interventies:

1. De x-as toont het percentage studenten dat wordt benaderd voor interventie, beginnend met degenen met het hoogste risico.
2. De y-as toont het percentage daadwerkelijke uitvallers dat met deze benaderingsstrategie zou worden bereikt.
3. De gestippelde diagonale lijn vertegenwoordigt een willekeurige selectie van studenten.
4. Gebieden waar de curve het steilste is, geven de meest efficiënte drempels voor interventie aan.

Je zou bijvoorbeeld kunnen ontdekken dat het benaderen van de top 20% studenten met het hoogste risico 60% van alle uitvallers bereikt - wat een verbetering van 3x ten opzichte van willekeurige selectie aangeeft. Deze informatie helpt bestuurders om geïnformeerde beslissingen te nemen over de toewijzing van middelen voor behoudprogramma's.

## Volgende stappen

De modelinterpretaties en visualisaties zijn nu opgeslagen in de map `data/modelled/interpreted`. In het volgende document maken we een samenvattend dashboard van onze bevindingen, gericht op studentenrangschikking en vroege interventiestrategieën.
