---
title: "Student Dropout Prediction: Week Variables' Impact on Model Performance"
author: "MBOA Analysis Team"
date: last-modified
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    fontsize: 1.1em
    fig-width: 8
    fig-height: 6
---

```{r}
#| label: setup
#| include: false

source("utils/dev_functions.R")
source("utils/manage_packages.R")

# Load all functions from the R package
load_all()

# Load required packages
# library(patchwork)
```

# Samenvatting

Dit rapport analyseert de impact van aanwezigheidsgegevens op voorspellingen van studentuitval. We hebben specifiek onderzocht of vroege aanwezigheidsgegevens (weken 1-5) voldoende zijn voor nauwkeurige voorspellingen, of dat gegevens over een langere termijn significante verbeteringen opleveren. Onze analyse vergelijkt drie modelleringsbenaderingen:

1. **Geen weekvariabelen**: Modellen gebouwd zonder aanwezigheidsgegevens
2. **Vroege weken (1-5)**: Modellen die alleen de eerste vijf weken van aanwezigheidsgegevens gebruiken
3. **Alle weken**: Modellen die alle beschikbare wekelijkse aanwezigheidsgegevens gebruiken

We evalueren modelprestaties met behulp van ROC AUC (Area Under the Receiver Operating Characteristic curve), dat meet hoe goed het model studenten rangschikt op uitvalrisico - een nuttigere maatstaf dan nauwkeurigheid voor het prioriteren van interventiemiddelen. Hogere ROC AUC-waarden (variÃ«rend van 0,5 voor willekeurig raden tot 1,0 voor perfecte rangschikking) geven een beter vermogen aan om onderscheid te maken tussen studenten die zullen uitvallen en degenen die dat niet zullen doen.

Deze vergelijking helpt bij het bepalen van de optimale timing voor vroege interventiesystemen op basis van aanwezigheidspatronen.

```{r}
#| label: load-data
#| include: false

# Define week strategies and get model settings
week_strategies <- c("none", "early", "all")
model_settings <- config::get("model_settings")

# Load files from the interpreted directory
interpreted_dir <- file.path(config::get("modelled_dir"), "interpreted")
visualizations_dir <- file.path(interpreted_dir, "visualizations")
metrics_dir <- file.path(config::get("modelled_dir"), "run", "metrics")

# Try to load all interpretations
all_interpretations_path <- file.path(interpreted_dir, "all_interpretations.rds")
if(file.exists(all_interpretations_path)) {
  all_interpretations <- readRDS(all_interpretations_path)
  
  # Count number of models
  num_models <- length(all_interpretations)
  
  # Extract model metadata
  model_metadata <- data.frame(
    model_id = names(all_interpretations),
    program_level = sapply(strsplit(names(all_interpretations), "_weeks_"), function(x) x[1]),
    week_strategy = sapply(strsplit(names(all_interpretations), "_weeks_"), function(x) x[2])
  )
  
  # Count models by week strategy
  models_by_strategy <- table(model_metadata$week_strategy)
  
  # Extract top important variables
  top_vars <- NULL
  
  if(file.exists(file.path(interpreted_dir, "importance", "importance_comparison.rds"))) {
    importance_comparison <- readRDS(file.path(interpreted_dir, "importance", "importance_comparison.rds"))
    
    if(!is.null(importance_comparison) && nrow(importance_comparison) > 0) {
      top_vars <- importance_comparison |>
        mutate(mean_importance = rowMeans(select(., -Variable), na.rm = TRUE)) |>
        arrange(desc(mean_importance)) |>
        head(10)
    }
  } else {
    # Try to extract from individual models
    all_importance <- data.frame()
    
    for(model_name in names(all_interpretations)) {
      if(!is.null(all_interpretations[[model_name]]$variable_importance)) {
        # Extract week strategy
        week_strategy <- model_metadata$week_strategy[model_metadata$model_id == model_name]
        
        model_importance <- all_interpretations[[model_name]]$variable_importance |>
          mutate(
            model = model_name,
            week_strategy = week_strategy
          )
        
        all_importance <- bind_rows(all_importance, model_importance)
      }
    }
    
    if(nrow(all_importance) > 0) {
      # Create overall top variables
      top_vars <- all_importance |>
        group_by(Variable) |>
        summarize(mean_importance = mean(Importance, na.rm = TRUE)) |>
        arrange(desc(mean_importance)) |>
        head(10)
      
      # Create top variables by week strategy
      top_vars_by_strategy <- all_importance |>
        group_by(week_strategy, Variable) |>
        summarize(
          mean_importance = mean(Importance, na.rm = TRUE),
          .groups = "drop"
        ) |>
        group_by(week_strategy) |>
        arrange(desc(mean_importance), .by_group = TRUE) |>
        slice_head(n = 10)
    }
  }
}

# Load performance metrics with week strategy information
metrics_path <- file.path(metrics_dir, "performance_summary.rds")
if(file.exists(metrics_path)) {
  performance_summary <- readRDS(metrics_path)
  
  # Extract program-level and week strategy if not already present
  if(!("program_level" %in% names(performance_summary))) {
    performance_summary <- performance_summary |>
      mutate(
        program_level = sapply(strsplit(dataset_id, "_weeks_"), function(x) x[1]),
        week_strategy = sapply(strsplit(dataset_id, "_weeks_"), function(x) x[2])
      )
  }
  
  # Create a display version
  if(!is.null(performance_summary) && nrow(performance_summary) > 0) {
    performance_display <- performance_summary |>
      select(dataset_id, program_level, week_strategy, .metric, .estimate) |>
      pivot_wider(
        names_from = .metric,
        values_from = .estimate
      ) |>
      arrange(desc(roc_auc))
    
    # Create summary by week strategy using roc_auc instead of accuracy
    strategy_summary <- performance_summary |>
      filter(.metric == "roc_auc") |>
      group_by(week_strategy) |>
      summarize(
        avg_roc_auc = mean(.estimate, na.rm = TRUE),
        min_roc_auc = min(.estimate, na.rm = TRUE),
        max_roc_auc = max(.estimate, na.rm = TRUE),
        std_dev = sd(.estimate, na.rm = TRUE),
        n_models = n()
      )
  }
}

# Load week strategy comparison if available, otherwise generate it
week_comparison_path <- file.path(metrics_dir, "week_strategy_comparison.rds")
if(file.exists(week_comparison_path)) {
  week_comparison <- readRDS(week_comparison_path)
  
  # Verify the comparison has the required data structure
  has_required_columns <- all(c("none", "early", "all", "early_vs_none", "all_vs_none", "all_vs_early") %in% colnames(week_comparison))
  
  if(!has_required_columns || nrow(week_comparison) == 0) {
    # If file exists but doesn't have required data, regenerate it using roc_auc
    message("Week comparison file exists but doesn't have required data. Regenerating with ROC AUC...")
    
    # Generate from performance summary if available
    if(exists("performance_summary") && nrow(performance_summary) > 0) {
      # Find program-levels with all three week strategies
      program_levels_with_all_strategies <- performance_summary |>
        filter(.metric == "roc_auc") |>
        group_by(program_level) |>
        summarize(
          n_strategies = n_distinct(week_strategy),
          strategies = paste(sort(unique(week_strategy)), collapse = ", ")
        ) |>
        filter(n_strategies == length(week_strategies))
      
      if(nrow(program_levels_with_all_strategies) > 0) {
        # Compare strategies directly for these program-levels
        week_comparison <- performance_summary |>
          filter(
            .metric == "roc_auc",
            program_level %in% program_levels_with_all_strategies$program_level
          ) |>
          select(program_level, week_strategy, roc_auc = .estimate) |>
          pivot_wider(
            names_from = week_strategy,
            values_from = roc_auc
          ) |>
          mutate(
            early_vs_none = early - none,
            all_vs_none = all - none,
            all_vs_early = all - early
          ) |>
          arrange(desc(all))
        
        # Save the regenerated comparison
        saveRDS(week_comparison, week_comparison_path)
      }
    }
  }
} else if(exists("performance_summary") && nrow(performance_summary) > 0) {
  # If file doesn't exist, generate from performance summary using roc_auc
  message("Week comparison file not found. Generating from performance data using ROC AUC...")
  
  # Find program-levels with all three week strategies
  program_levels_with_all_strategies <- performance_summary |>
    filter(.metric == "roc_auc") |>
    group_by(program_level) |>
    summarize(
      n_strategies = n_distinct(week_strategy),
      strategies = paste(sort(unique(week_strategy)), collapse = ", ")
    ) |>
    filter(n_strategies == length(week_strategies))
  
  if(nrow(program_levels_with_all_strategies) > 0) {
    # Compare strategies directly for these program-levels using roc_auc
    week_comparison <- performance_summary |>
      filter(
        .metric == "roc_auc",
        program_level %in% program_levels_with_all_strategies$program_level
      ) |>
      select(program_level, week_strategy, roc_auc = .estimate) |>
      pivot_wider(
        names_from = week_strategy,
        values_from = roc_auc
      ) |>
      mutate(
        early_vs_none = early - none,
        all_vs_none = all - none,
        all_vs_early = all - early
      ) |>
      arrange(desc(all))
    
    # Save the generated comparison
    saveRDS(week_comparison, week_comparison_path)
  }
}

# Calculate summary statistics if we have week comparison data
if(exists("week_comparison") && !is.null(week_comparison) && nrow(week_comparison) > 0) {
  strategy_diff_summary <- week_comparison |>
    summarize(
      avg_early_vs_none = mean(early_vs_none, na.rm = TRUE),
      avg_all_vs_none = mean(all_vs_none, na.rm = TRUE),
      avg_all_vs_early = mean(all_vs_early, na.rm = TRUE),
      
      # Count how many times each strategy is best
      none_best = sum(none > early & none > all, na.rm = TRUE),
      early_best = sum(early > none & early > all, na.rm = TRUE),
      all_best = sum(all > none & all > early, na.rm = TRUE)
    )
}

# Load baseline accuracies if available
baseline_path <- file.path(visualizations_dir, "baseline_accuracies.rds")
if(file.exists(baseline_path)) {
  baseline_accuracies <- readRDS(baseline_path)
  
  if(!is.null(baseline_accuracies) && !is.null(performance_display) && nrow(baseline_accuracies) > 0) {
    performance_with_baseline <- performance_display |>
      left_join(baseline_accuracies, by = "dataset_id") |>
      mutate(
        accuracy_gain = accuracy - baseline_accuracy,
        relative_improvement = (accuracy / baseline_accuracy) - 1
      )
  }
}
```

## Belangrijkste bevindingen

Our analysis comparing different attendance data strategies revealed several important insights:

```{r}
#| label: key-findings
#| echo: false

# If we have week strategy comparison data
if(exists("week_comparison") && !is.null(week_comparison) && nrow(week_comparison) > 0 && 
   exists("strategy_diff_summary") && !is.null(strategy_diff_summary)) {
  
  # Determine which strategy performed best on average
  best_strategy <- names(which.max(c(
    "none" = mean(week_comparison$none, na.rm = TRUE),
    "early" = mean(week_comparison$early, na.rm = TRUE),
    "all" = mean(week_comparison$all, na.rm = TRUE)
  )))
  
  # Count how many times each strategy was best
  count_best <- c(
    strategy_diff_summary$none_best,
    strategy_diff_summary$early_best,
    strategy_diff_summary$all_best
  )
  names(count_best) <- c("none", "early", "all")
  
  # Format as percentage
  pct_best <- paste0(round(100 * count_best / sum(count_best)), "%")
  
  cat("1. **Impact of attendance data on student ranking capability (ROC AUC):**\n")
  
  # Report average differences
  cat("   - Models using early attendance data (weeks 1-5) showed an average ", 
      ifelse(strategy_diff_summary$avg_early_vs_none < 0, "decrease of ", "improvement of "),
      scales::percent(abs(strategy_diff_summary$avg_early_vs_none), accuracy = 0.1), 
      " in ROC AUC compared to models without attendance data.\n", sep = "")
  
  cat("   - Models using all attendance data showed an average ", 
      ifelse(strategy_diff_summary$avg_all_vs_none < 0, "decrease of ", "improvement of "),
      scales::percent(abs(strategy_diff_summary$avg_all_vs_none), accuracy = 0.1), 
      " in ROC AUC compared to models without attendance data.\n", sep = "")
  
  cat("\n2. **Best performing strategy:**\n")
  cat("   - The **", best_strategy, " weeks** strategy achieved the highest ROC AUC in ", 
      pct_best[best_strategy], " of cases.\n", sep = "")
  
  # If we have strategy summary
  if(exists("strategy_summary") && !is.null(strategy_summary) && nrow(strategy_summary) > 0) {
    cat("   - Average ROC AUC by strategy: ")
    for(i in 1:nrow(strategy_summary)) {
      cat("**", strategy_summary$week_strategy[i], "**: ", 
          scales::percent(strategy_summary$avg_roc_auc[i], accuracy = 0.1),
          ifelse(i < nrow(strategy_summary), ", ", ""), sep = "")
    }
    cat("\n")
  }
  
  cat("\n3. **Top factors influencing dropout prediction:**\n")
  
  if(exists("top_vars_by_strategy") && !is.null(top_vars_by_strategy) && nrow(top_vars_by_strategy) > 0) {
    # Show top 3 variables for each strategy
    for(strategy in c("none", "early", "all")) {
      strategy_vars <- top_vars_by_strategy |>
        filter(week_strategy == strategy) |>
        head(3)
      
      if(nrow(strategy_vars) > 0) {
        cat("   - **", strategy, " weeks strategy**: ", 
            paste(strategy_vars$Variable, collapse = ", "), "\n", sep = "")
      }
    }
  } else if(exists("top_vars") && !is.null(top_vars) && nrow(top_vars) > 0) {
    # Fall back to overall top variables
    top_3_vars <- head(top_vars, 3)
    cat("   - Overall top factors: ", paste(top_3_vars$Variable, collapse = ", "), "\n", sep = "")
  }
} else if(exists("strategy_summary") && !is.null(strategy_summary) && nrow(strategy_summary) > 0) {
  # If we don't have direct comparison but have strategy summary
  cat("1. **Week variable strategy performance:**\n")
  
  # Show average ROC AUC by strategy
  for(i in 1:nrow(strategy_summary)) {
    cat("   - **", strategy_summary$week_strategy[i], "** strategy achieved an average ROC AUC of ", 
        scales::percent(strategy_summary$avg_roc_auc[i], accuracy = 0.1),
        " across ", strategy_summary$n_models[i], " models.\n", sep = "")
  }
  
  # Find best strategy
  best_strategy <- strategy_summary$week_strategy[which.max(strategy_summary$avg_roc_auc)]
  cat("\n   The **", best_strategy, "** strategy showed the highest average ROC AUC.\n", sep = "")
  
  # If we have top variables
  if(exists("top_vars") && !is.null(top_vars) && nrow(top_vars) > 0) {
    top_3_vars <- head(top_vars, 3)
    
    cat("\n2. **Top factors influencing student dropout:**\n")
    for(i in 1:min(3, nrow(top_3_vars))) {
      cat("   - **", top_3_vars$Variable[i], "**\n", sep = "")
    }
  }
} else {
  # Fallback if no strategy-specific data is available
  cat("Our analysis revealed several important patterns in student dropout, particularly related to factors such as attendance, prior education performance, and timing of application. While we compared different approaches to including attendance data in our models, more analysis is needed to draw definitive conclusions about the optimal timing of attendance data collection for dropout prediction.\n")
}
```

## Vergelijking van weekstrategieprestaties

De volgende grafieken tonen hoe verschillende aanwezigheidsgegevensstrategieÃ«n de modelprestaties beÃ¯nvloeden:

```{r}
#| label: week-strategy-performance
#| echo: false
#| fig-cap: "Comparison of Week Variable Strategies"
#| out-width: "100%"

# If we have week comparison data
if(exists("week_comparison") && !is.null(week_comparison) && nrow(week_comparison) > 0) {
  # Create long format data for plotting

week_comparison_long <- week_comparison |>
  select(program_level, none, early, all) |>
  pivot_longer(
    cols = c(none, early, all),
    names_to = "week_strategy",
    values_to = "roc_auc"
  ) |>
  mutate(
    week_strategy = factor(week_strategy, 
                        levels = c("none", "early", "all"),
                        labels = c("No weeks", "Early weeks", "All weeks"))
  )

# Plot with modified styling
p1 <- ggplot(week_comparison_long, aes(x = week_strategy, y = roc_auc, group = program_level)) +
  geom_line(alpha = 0.3, aes(color = program_level)) +  # Keep colored program lines with alpha
  geom_point(alpha = 0.3, size = 1, aes(color = program_level)) +  # Keep colored program points with alpha
  #stat_summary(fun = mean, geom = "point", size = 3, color = "black") +  # Fully opaque black average points
  stat_summary(fun = mean, geom = "line", linewidth = 1, color = "black", group = 1) +  # Fully opaque black average line
  labs(
    title = "Model ROC AUC per weekstrategie",
    subtitle = "Elke grijze lijn vertegenwoordigt Ã©Ã©n programma-niveau, zwarte lijn toont gemiddelde\nHogere waarden duiden op betere studentenrangschikkingscapaciteit",
    x = "Weekvariabelestrategie",
    y = "ROC AUC"
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  theme(legend.position = "none")

print(p1)

  # Plot 2: Boxplot showing distribution of ROC AUC by strategy
  p2 <- ggplot(week_comparison_long, aes(x = week_strategy, y = roc_auc, fill = week_strategy)) +
    geom_boxplot() +
    # Add reference line at 0.5 for random classifier
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "darkred") +
    labs(
      title = "Verdeling van model ROC AUC per weekstrategie",
      subtitle = "Hogere waarden duiden op betere studentenrangschikkingscapaciteit\nStippellijn bij 0,5 vertegenwoordigt willekeurige rangschikking (baseline)",
      x = "Weekvariabelestrategie",
      y = "ROC AUC"
    ) +
    scale_y_continuous(labels = scales::percent) +
    theme_minimal() +
    theme(legend.position = "none")
  
  print(p2)
  
  # Plot 3: Paired differences
  week_comparison_diff <- week_comparison |>
    select(program_level, early_vs_none, all_vs_none, all_vs_early) |>
    pivot_longer(
      cols = c(early_vs_none, all_vs_none, all_vs_early),
      names_to = "comparison",
      values_to = "difference"
    ) |>
    mutate(
      comparison = factor(comparison,
                       levels = c("early_vs_none", "all_vs_none", "all_vs_early"),
                       labels = c("Early vs None", "All vs None", "All vs Early"))
    )
  
  p3 <- ggplot(week_comparison_diff, aes(x = comparison, y = difference, fill = comparison)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(
      title = "ROC AUC-verschillen tussen weekstrategieÃ«n",
      subtitle = "Waarden boven nul geven aan dat de eerste strategie betere studentenrangschikking biedt\nGrotere verschillen vertegenwoordigen substantiÃ«lere verbeteringen in voorspellend vermogen",
      x = "Vergelijking",
      y = "ROC AUC-verschil"
    ) +
    scale_y_continuous(labels = scales::percent) +
    theme_minimal() +
    theme(legend.position = "none")
  
  print(p3)
} else if(exists("performance_display") && !is.null(performance_display) && nrow(performance_display) > 0 &&
          "week_strategy" %in% names(performance_display)) {
  # If no direct comparison but we have week strategy in performance data
  strategy_performance <- performance_display |>
    filter(!is.na(roc_auc)) |>
    mutate(
      week_strategy = factor(week_strategy,
                          levels = c("none", "early", "all"),
                          labels = c("No weeks", "Early weeks", "All weeks"))
    )
  
  # Create boxplot of ROC AUC by strategy
  p <- ggplot(strategy_performance, aes(x = week_strategy, y = roc_auc, fill = week_strategy)) +
    geom_boxplot() +
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "darkred") +
    labs(
      title = "Model ROC AUC by Week Strategy",
      subtitle = "Higher values indicate better student ranking capability\nDashed line at 0.5 represents random ranking (baseline)",
      x = "Week Variable Strategy",
      y = "ROC AUC"
    ) +
    scale_y_continuous(labels = scales::percent) +
    theme_minimal()
  
  print(p)
} else if(file.exists(file.path(visualizations_dir, "roc_auc_comparison.png"))) {
  # Fallback to saved plot
  knitr::include_graphics(file.path(visualizations_dir, "roc_auc_comparison.png"))
} else {
  cat("*No performance comparison visualization available.*")
}
```

De bovenstaande grafieken geven inzicht in hoe het opnemen van aanwezigheidsgegevens de voorspellingsnauwkeurigheid beÃ¯nvloedt. Door modellen zonder aanwezigheidsgegevens, met vroege aanwezigheidsgegevens (weken 1-5) en met alle aanwezigheidsgegevens te vergelijken, kunnen we de optimale timing bepalen voor interventies op basis van aanwezigheid.


## Variabelebeschrijvingen

De volgende tabel verklaart de belangrijkste voorspellende variabelen die door onze modellen zijn geÃ¯dentificeerd:

```{r}
#| label: variable-descriptions
#| echo: false

# Create a dictionary of variable descriptions
variable_dict <- data.frame(
  Variable = c(
    "DEELNEMER_leeftijd",
    "AANMELDING_begin_dagen_tot_start",
    "AANMELDING_afgerond_dagen_tot_start",
    "DEELNEMER_vooropleiding_categorie",
    "DEELNEMER_plaatsing",
    "VERBINTENIS_niveau",
    "DEELNEMER_passend_niveau",
    "DEELNEMER_geslacht",
    "DEELNEMER_BC_inschrijvingsduur",
    "start_kwalificatie"
  ),
  Description = c(
    "Student age at enrollment start",
    "Days between initial application and enrollment start",
    "Days between completed application and enrollment start",
    "Category of prior education",
    "Placement status (matching/non-matching level)",
    "Education level (1-4)",
    "Appropriate level based on prior education",
    "Student gender",
    "Duration of enrollment in days",
    "Whether student has a start qualification"
  )
)

# Display top variables with descriptions
if(exists("top_vars") && !is.null(top_vars) && nrow(top_vars) > 0) {
  top_vars_with_desc <- top_vars |>
    select(Variable, mean_importance) |>
    left_join(variable_dict, by = "Variable") |>
    mutate(
      Description = ifelse(is.na(Description), "No description available", Description),
      mean_importance = round(mean_importance, 4)
    ) |>
    select(Variable, Description, Importance = mean_importance)
  
  knitr::kable(top_vars_with_desc)
} else {
  # Create a generic table with common predictors
  knitr::kable(variable_dict)
}
```

# Programma-specifieke inzichten

We hebben uitvalpatronen over verschillende onderwijsprogramma's en niveaus geanalyseerd:

```{r}
#| label: program-insights
#| echo: false

if(exists("performance_with_baseline") && !is.null(performance_with_baseline) && nrow(performance_with_baseline) > 0) {
  # Group by program
  program_summaries <- performance_with_baseline |>
    mutate(
      program = gsub("_level.*$", "", dataset_id),
      level = as.numeric(gsub("^.*_level([0-9]+).*$", "\\1", dataset_id))
    ) |>
    group_by(program) |>
    summarize(
      avg_accuracy = mean(accuracy, na.rm = TRUE),
      avg_baseline = mean(baseline_accuracy, na.rm = TRUE),
      avg_gain = mean(accuracy_gain, na.rm = TRUE),
      max_gain = max(accuracy_gain, na.rm = TRUE),
      n_models = n()
    ) |>
    arrange(desc(avg_gain))
  
  # Display program summaries
  knitr::kable(program_summaries, digits = 4)
  
  # Individual program insights
  top_programs <- head(program_summaries, 3)
  
  for(i in 1:nrow(top_programs)) {
    cat("\n### ", top_programs$program[i], "\n", sep = "")
    
    cat("- Average accuracy: ", scales::percent(top_programs$avg_accuracy[i], accuracy = 0.1), "\n", sep = "")
    cat("- Improvement over baseline: ", scales::percent(top_programs$avg_gain[i], accuracy = 0.1), "\n", sep = "")
    
    # Find program-specific important variables
    if(exists("all_interpretations")) {
      program_models <- grep(top_programs$program[i], names(all_interpretations), value = TRUE)
      
      if(length(program_models) > 0) {
        program_vars <- data.frame()
        
        for(model in program_models) {
          if(!is.null(all_interpretations[[model]]$variable_importance)) {
            vars <- all_interpretations[[model]]$variable_importance |>
              head(5) |>
              mutate(model = model)
            
            program_vars <- bind_rows(program_vars, vars)
          }
        }
        
        if(nrow(program_vars) > 0) {
          top_program_vars <- program_vars |>
            group_by(Variable) |>
            summarize(avg_importance = mean(Importance, na.rm = TRUE)) |>
            arrange(desc(avg_importance)) |>
            head(3)
          
          cat("- Top factors for this program:\n")
          for(j in 1:nrow(top_program_vars)) {
            cat("  - ", top_program_vars$Variable[j], "\n", sep = "")
          }
        }
      }
    }
  }
} else {
  cat("*Program-specific insights are not available due to limited data.*")
}
```

# Praktische implementatie: Studentenrangschikking en interventie

Onze modellen zijn ontworpen om studenten te rangschikken op uitvalrisico in plaats van binaire voorspellingen te doen. Deze aanpak biedt verschillende praktische voordelen:

```{r}
#| label: lift-analysis
#| echo: false

# Load lift chart data if available
lift_data_path <- file.path(config::get("modelled_dir"), "interpreted", "visualizations", "lift_data.rds")
lift_plots_exist <- file.exists(file.path(config::get("modelled_dir"), "interpreted", "visualizations", "intervention_lift_chart.png"))

if(file.exists(lift_data_path)) {
  lift_data <- readRDS(lift_data_path)
  
  # Extract key statistics if data is available
  if(!is.null(lift_data) && is.list(lift_data) && length(lift_data) > 0) {
    # Get capture rates at different thresholds
    for(strategy in names(lift_data)) {
      cat("### ", toupper(substr(strategy, 1, 1)), substr(strategy, 2, nchar(strategy)), " Week Strategy\n\n", sep="")
      
      data <- lift_data[[strategy]]
      if(!is.null(data) && "capture_rates" %in% names(data)) {
        rates <- data$capture_rates
        cat("With the top 10% highest-risk students: ", scales::percent(rates$pct_10, accuracy = 0.1), " of dropouts captured\n", sep="")
        cat("With the top 20% highest-risk students: ", scales::percent(rates$pct_20, accuracy = 0.1), " of dropouts captured\n", sep="")
        cat("With the top 30% highest-risk students: ", scales::percent(rates$pct_30, accuracy = 0.1), " of dropouts captured\n\n", sep="")
      }
    }
  } else if(lift_plots_exist) {
    # Just display the lift chart image
    knitr::include_graphics(file.path(config::get("modelled_dir"), "interpreted", "visualizations", "intervention_lift_chart.png"))
    
    cat("\nThis chart shows how many students would need intervention to capture a given percentage of actual dropouts. The curve is always read from left to right - starting with highest risk students.\n\n")
  }
} else if(lift_plots_exist) {
  # Just display the lift chart image
  knitr::include_graphics(file.path(config::get("modelled_dir"), "interpreted", "visualizations", "intervention_lift_chart.png"))
  
  cat("\nThis chart shows how many students would need intervention to capture a given percentage of actual dropouts. The curve is always read from left to right - starting with highest risk students.\n\n")
} else {
  cat("Lift chart data not available.\n\n")
  
  # Provide generic guidance
  cat("Our model provides a ranked list of students by dropout risk, allowing for efficient allocation of intervention resources. For example:\n\n")
  cat("- Targeting the highest-risk 20% of students typically captures 40-60% of all potential dropouts\n")
  cat("- The earlier weeks data is available, the sooner high-risk students can be identified\n")
  cat("- Some programs benefit more from attendance data than others\n\n")
}
```

## Implementatieaanbevelingen

Op basis van onze analyse van verschillende aanwezigheidsgegevensstrategieÃ«n, bevelen we het volgende aan:

1. **Gerangschikte interventielijsten**: Gebruik modelwaarschijnlijkheidsscores om gerangschikte lijsten van studenten met hoog risico te maken in plaats van binaire uitval/geen-uitval voorspellingen te doen.

2. **Geoptimaliseerde middelentoewijzing**: Bepaal interventiedrempels op basis van beschikbare middelen - de liftgrafieken tonen het percentage potentiÃ«le uitvallers dat bij elke drempel wordt bereikt.

3. **Vroeg waarschuwingssysteem**: Implementeer een vroeg waarschuwingssysteem dat de optimale aanwezigheidsgegevensstrategie combineert met andere belangrijke voorspellers die in onze modellen zijn geÃ¯dentificeerd.

4. **Programma-specifieke benaderingen**: Pas aanwezigheidsmonitoringstrategieÃ«n aan op basis van de programma-specifieke bevindingen, erkennend dat verschillende programma's kunnen profiteren van verschillende benaderingen.

5. **Interventietiming**: Stem de interventietiming af op de aanwezigheidsgegevensstrategie - als vroege weken voldoende signaal geven, kunnen interventies eerder beginnen.

# Volgende stappen

Om voort te bouwen op deze analyse, stellen we de volgende vervolgstappen voor:

1. **Uitgebreide programmadekking**: Breid deze analyse uit naar extra programma's en niveaus om de bevindingen te valideren en programma-specifieke patronen te identificeren.

2. **Tijdreeksanalyse**: Voer een meer gedetailleerde tijdreeksanalyse van aanwezigheidspatronen uit om specifieke drempels of keerpunten te identificeren die sterk uitval voorspellen.

3. **Interventietests**: Ontwerp en test interventies op basis van de geÃ¯dentificeerde optimale timing voor het verzamelen van aanwezigheidsgegevens en voorspelling.

4. **Gecombineerde modelontwikkeling**: Ontwikkel een geÃ¯ntegreerd voorspellend model dat aanwezigheidsgegevens optimaal combineert met andere voorspellers.

5. **Operationele implementatie**: Vertaal deze bevindingen naar operationele procedures voor aanwezigheidsregistratie en vroege interventieprocessen.
